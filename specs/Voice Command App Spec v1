# VoiceControl Implementation Specification

## Project Overview

VoiceControl is a voice-controlled text manipulation app for macOS with three core modes:
- **Dictation Mode**: Real-time speech-to-text
- **Edit Mode**: AI-powered text transformations
- **Command Mode**: Voice-activated system commands

## Technical Stack

- **Language**: Swift 5.9+
- **UI Framework**: SwiftUI
- **Minimum macOS**: 13.0 (Ventura)
- **APIs**: OpenAI Whisper (transcription), OpenAI GPT-4 (text editing)

## Project Structure

```
VoiceControl/
├── VoiceControlApp.swift          # Main app entry
├── Config/
│   └── Config.swift              # Centralized configuration
├── Core/
│   ├── AudioEngine.swift         # Mic capture & processing
│   ├── WhisperService.swift      # OpenAI Whisper integration
│   ├── GPTService.swift          # OpenAI GPT integration
│   └── AccessibilityBridge.swift # Text manipulation via AXUIElement
├── Features/
│   ├── Dictation/
│   │   ├── DictationManager.swift
│   │   └── DictationHUD.swift
│   ├── Edit/
│   │   ├── EditManager.swift
│   │   └── EditHUD.swift
│   └── Command/
│       ├── CommandManager.swift
│       ├── CommandMatcher.swift
│       └── CommandHUD.swift
├── Models/
│   └── Command.swift
├── Resources/
│   └── commands.json             # Command definitions
└── Utils/
    ├── HotkeyManager.swift       # Global hotkey handling
    └── TextSelection.swift       # Text selection utilities
```

## Core Components

### 1. Configuration (Config.swift)

```swift
struct Config {
    // Keyboard Shortcuts - centralized for easy modification
    static let dictationHotkey = "⌥⌘D"
    static let editHotkey = "⌥⌘E"
    static let commandHotkey = "⌥⌘C"
    
    // API Configuration
    static let openAIKey = ProcessInfo.processInfo.environment["OPENAI_API_KEY"] ?? ""
    static let whisperModel = "whisper-1"
    static let gptModel = "gpt-4-turbo-preview"
    
    // Behavior
    static let silenceThreshold: TimeInterval = 1.0
    static let fuzzyMatchThreshold: Double = 0.85
}
```

### 2. Audio Engine

- Use `AVAudioEngine` for microphone capture
- Stream audio chunks to Whisper API
- Handle start/stop based on hotkey press or silence detection
- Basic error handling for microphone permissions

### 3. Whisper Service

- Implement streaming transcription using OpenAI Whisper API
- Support model size selection (future enhancement)
- Return partial results for HUD display
- Handle network errors gracefully

### 4. Accessibility Bridge

Key capabilities:
- Get current text selection from any app
- Replace selected text
- Move cursor position
- Execute rich text navigation commands

Implementation approach:
```swift
class AccessibilityBridge {
    func getCurrentSelection() -> (text: String, range: NSRange)?
    func replaceSelection(with text: String)
    func moveCursor(to position: CursorPosition)
    func selectText(matching pattern: SelectionPattern)
}
```

### 5. HUD Windows

All HUDs should:
- Be discrete overlay windows anchored at bottom of screen
- Stay out of the way of user's work area
- Use modern macOS visual effects (vibrancy/blur)
- Auto-dismiss after action completion

**Dictation HUD**: Shows live transcription with pulsing mic indicator
**Edit HUD**: Shows original text → transformed text preview
**Command HUD**: Shows recognized command or disambiguation picker

Design specifications:
- Position: Centered horizontally, 20px from bottom edge
- Width: Dynamic based on content, max 600px
- Height: Compact, ~60-80px for single-line content
- Style: Dark vibrancy effect with rounded corners
- Animation: Smooth fade in/out, subtle slide up from bottom

## Command System

### Command Model

```swift
struct Command: Codable {
    let id: String
    let phrases: [String]  // Variations for fuzzy matching
    let action: CommandAction
    let category: CommandCategory
}

enum CommandAction: Codable {
    case selectText(SelectionType)
    case moveCursor(Direction, Unit)
    case systemAction(String)
    case appCommand(appId: String, command: String)
}

enum SelectionType: String, Codable {
    case word, sentence, paragraph, line, all
    case next, previous
    case toEndOfLine, toStartOfLine
    // ... etc
}
```

### Initial Command Set (commands.json)

```json
{
  "commands": [
    {
      "id": "select_word",
      "phrases": ["select word", "highlight word"],
      "action": {"type": "selectText", "selection": "word"}
    },
    {
      "id": "select_next_sentence",
      "phrases": ["select next sentence", "highlight next sentence"],
      "action": {"type": "selectText", "selection": "nextSentence"}
    },
    {
      "id": "move_to_end",
      "phrases": ["go to end", "move to end", "cursor to end"],
      "action": {"type": "moveCursor", "direction": "end", "unit": "document"}
    },
    {
      "id": "delete_word",
      "phrases": ["delete word", "remove word"],
      "action": {"type": "systemAction", "key": "⌥⌫"}
    }
    // ... more commands
  ]
}
```

### Fuzzy Matching

Use a simple string similarity algorithm (Levenshtein distance or similar) to match spoken phrases to commands. If confidence < 0.85, show disambiguation picker.

## Implementation Flow

### Dictation Mode Flow

1. User presses `⌥⌘D` → Start audio capture
2. Show DictationHUD with mic indicator
3. Stream audio to Whisper → Display partials in HUD
4. On second `⌥⌘D` press or 1s silence → Stop capture
5. Insert final text at cursor position
6. Dismiss HUD

### Edit Mode Flow

1. User selects text in any app
2. User presses `⌥⌘E` → Capture selection via Accessibility API
3. Show EditHUD with selected text
4. User speaks transformation instruction
5. Press `⌥⌘E` again → Send to GPT-4
6. Show transformed text preview
7. Replace selection with result
8. Dismiss HUD

### Command Mode Flow

1. User presses `⌥⌘C` → Start listening
2. Show CommandHUD
3. User speaks command
4. Fuzzy match against command inventory
5. If match > 0.85 → Execute immediately
6. If match < 0.85 → Show picker with top matches
7. Execute selected command
8. Dismiss HUD

## Error Handling

- **No API Key**: Show alert on first run with instructions to set `OPENAI_API_KEY` environment variable
- **Network Errors**: Show brief error toast, don't crash
- **Accessibility Denied**: Show alert with instructions to grant permissions in System Settings
- **Microphone Denied**: Show alert with fix instructions
- **API Rate Limits**: Show user-friendly message, implement basic exponential backoff

## Development Priorities

1. **Phase 1**: Basic dictation with hotkey toggle
2. **Phase 2**: Accessibility bridge for text manipulation
3. **Phase 3**: Command system with fuzzy matching
4. **Phase 4**: Edit mode with GPT integration
5. **Phase 5**: Polish HUD designs and animations

## Testing Approach

- Test with common macOS apps: TextEdit, Safari, Xcode, VS Code
- Verify accessibility API works across different text fields
- Test command fuzzy matching with various phrasings
- Ensure hotkeys don't conflict with system shortcuts

## Future Considerations

- Command editing UI (Phase 2)
- Multi-step command chains (Phase 2)
- App launcher integration (Phase 3)
- Offline fallback with local Whisper model
- User-configurable hotkeys preference pane